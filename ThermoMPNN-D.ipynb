{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suzheng/ThermoMPNN-D/blob/main/ThermoMPNN-D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPU-s3e_ex-I"
      },
      "source": [
        "# <center>**This is the Colab implementation of ThermoMPNN-D**</center>\n",
        "\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1qXMpih7MLeZfRDZF9-iYSlL6SXEY3FdS'></center>\n",
        "\n",
        "---\n",
        "\n",
        "ThermoMPNN-D is an updated version of ThermoMPNN for predicting double point mutations. It was trained on an augmented version of the Megascale double mutant dataset. It is state-of-the-art at predicting stabilizing double mutations.\n",
        "\n",
        "For convenience, we also provide a single-mutant ThermoMPNN model and an \"additive\" model that finds mutation pairs in a naive fashion by ignoring epistatic interactions. For details, see the [ThermoMPNN-D paper](https://doi.org/10.1002/pro.70003).\n",
        "\n",
        "### **COLAB TIPS:**\n",
        "- The cells of this notebook are meant to be executed *in order*, so users should start from the top and work their way down.\n",
        "- Executable cells can be run by clicking the PLAY button (>) that appears when you hover over each cell, or by using **Shift+Enter**.\n",
        "- Make sure GPU is enabled by checking `Runtime` -> `Change Runtime Type`\n",
        "  - Make sure that `Runtime type` is set to `Python 3`\n",
        "  - Make sure that `Hardware accelerator` is set to `GPU`\n",
        "  - Click `Save` to confirm\n",
        "\n",
        "- If the notebook freezes up or otherwise crashes, go to `Runtime` -> `Restart Runtime` and try again.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zqSoIY9hfaae"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "#@title # 1. Set up **ThermoMPNN environment**\n",
        "#@markdown Import ThermoMPNN and its dependencies to this session. This may take a minute or two.\n",
        "\n",
        "#@markdown You only need to do this once *per session*. To re-run ThermoMPNN on a new protein, you may start on Step 3.\n",
        "\n",
        "# cleaning out any remaining data\n",
        "!cd /content\n",
        "!rm -rf /content/ThermoMPNN-D\n",
        "!rm -rf /content/sample_data\n",
        "!rm /content/*.pdb\n",
        "!rm /content/*.csv\n",
        "\n",
        "# import ThermoMPNN-D github repo\n",
        "import os\n",
        "if not os.path.exists(\"/content/ThermoMPNN-D\"):\n",
        "  !git clone https://github.com/suzheng/ThermoMPNN-D.git\n",
        "  %cd /content/ThermoMPNN-D\n",
        "\n",
        "# downloading various dependencies - add more if needed later\n",
        "! pip install omegaconf wandb pytorch-lightning biopython\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3Lgz5km3gFyv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#@title # **2. Set up ThermoMPNN imports and functions**\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from urllib import request\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "from google.colab._message import MessageError\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "tMPNN_path = '/content/ThermoMPNN-D'\n",
        "if tMPNN_path not in sys.path:\n",
        "  sys.path.append(tMPNN_path)\n",
        "\n",
        "\n",
        "def download_pdb(pdbcode, datadir, downloadurl=\"https://files.rcsb.org/download/\"):\n",
        "    \"\"\"\n",
        "    Downloads a PDB file from the Internet and saves it in a data directory.\n",
        "    :param pdbcode: The standard PDB ID e.g. '3ICB' or '3icb'\n",
        "    :param datadir: The directory where the downloaded file will be saved\n",
        "    :param downloadurl: The base PDB download URL, cf.\n",
        "        `https://www.rcsb.org/pages/download/http#structures` for details\n",
        "    :return: the full path to the downloaded PDB file or None if something went wrong\n",
        "    \"\"\"\n",
        "\n",
        "    pdbfn = pdbcode + \".pdb\"\n",
        "    url = downloadurl + pdbfn\n",
        "    outfnm = os.path.join(datadir, pdbfn)\n",
        "    try:\n",
        "        request.urlretrieve(url, outfnm)\n",
        "        return outfnm\n",
        "    except Exception as err:\n",
        "        print(str(err), file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "def drop_cysteines(df, mode):\n",
        "  \"\"\"Drop any mutations to Cys\"\"\"\n",
        "\n",
        "  if mode.lower() == 'single':\n",
        "    aatype_to = df['Mutation'].str[-1].values\n",
        "    is_cys = aatype_to == \"C\"\n",
        "    df = df.loc[~is_cys].reset_index(drop=True)\n",
        "\n",
        "  elif mode.lower() == 'additive' or mode.lower() == 'epistatic':\n",
        "    muts = df['Mutation'].str.split(':', n=2, expand=True).values # [N, 2]\n",
        "    is_cys = []\n",
        "    for m in muts:\n",
        "      mut1, mut2 = m\n",
        "      is_cys.append(mut1.endswith(\"C\") or mut2.endswith(\"C\"))\n",
        "\n",
        "    is_cys = np.array(is_cys)\n",
        "    df = df.loc[~is_cys].reset_index(drop=True)\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid mode {mode} selected!\")\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "WPhbnMAHf1qL"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "#@title # **3. Upload or Fetch Input Data**\n",
        "\n",
        "#@markdown ## You may either specify a PDB code to fetch or upload a custom PDB file.<br><br>\n",
        "\n",
        "# -------- Collecting Settings for ThermoMPNN run --------- #\n",
        "\n",
        "!rm /content/*.pdb &> /dev/null\n",
        "\n",
        "#@markdown PDB code (example: 1PGA):\n",
        "PDB = \"6vxx\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown -------\n",
        "\n",
        "#@markdown Upload Custom PDB?\n",
        "Custom = False #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown NOTE: If enabled, a `Choose files` button will appear at the bottom of this cell once this cell is run.\n",
        "\n",
        "#@markdown -----\n",
        "\n",
        "#@markdown Chain(s) of Interest (example: A,B,C):\n",
        "Chains = \"A,B,C\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown If left empty, all chains will be used.\n",
        "\n",
        "# try to upload the PDB file to Colab servers\n",
        "if Custom:\n",
        "  try:\n",
        "    uploaded_pdb = files.upload()\n",
        "    for fn in uploaded_pdb.keys():\n",
        "      PDB = os.path.basename(fn)\n",
        "      if not PDB.endswith('.pdb'):\n",
        "        raise ValueError(f\"Uploaded file {PDB} does not end in '.pdb'. Please check and rename file as needed.\")\n",
        "      os.rename(fn, os.path.join(\"/content/\", PDB))\n",
        "      pdb_file = os.path.join(\"/content/\", PDB)\n",
        "  except (MessageError, FileNotFoundError):\n",
        "    print('\\n', '*' * 100, '\\n')\n",
        "    print('Sorry, your input file failed to upload. Please try the backup upload procedure (next cell).')\n",
        "\n",
        "else:\n",
        "  try:\n",
        "    fn = download_pdb(PDB, \"/content/\")\n",
        "    if fn is None:\n",
        "      raise ValueError(\"Failed to fetch PDB from RSCB. Please double-check PDB code and try again.\")\n",
        "    else:\n",
        "      pdb_file = fn\n",
        "  except HTTPError:\n",
        "    raise HTTPError(f\"No protein with code {PDB} exists in RSCB PDB. Please double-check PDB code and try again.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjdyW_tmglI1",
        "outputId": "552ef0f2-8a2c-4c44-c6e6-e356b7ae499b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully uploaded PDB file 1bvc.pdb\n"
          ]
        }
      ],
      "source": [
        "#@title # **3. Backup Data Upload (ONLY needed if initial upload failed)**\n",
        "\n",
        "#@markdown ## Colab automatic file uploads are not very reliable. If your file failed to upload automatically, you can do so manually by following these steps.<br><br>\n",
        "\n",
        "#@markdown #### 1. Click the \"Files\" icon on the left toolbar. This will open the Colab server file folder.\n",
        "\n",
        "#@markdown #### 2. The only thing in this folder should be \"ThermoMPNN\" directory. If any other files are in here, delete them.\n",
        "\n",
        "#@markdown #### 3. Click the \"Upload to session storage\" button under the \"Files\" header. Choose your file for upload.\n",
        "\n",
        "#@markdown #### 4. Run this cell. ThermoMPNN will find your file in session storage and use it.\n",
        "\n",
        "#@markdown ------\n",
        "\n",
        "#@markdown Chain(s) of Interest (example: A,B,C):\n",
        "Chains = \"\" #@param {type:\"string\"}\n",
        "#@markdown If left empty, all chains will be used.\n",
        "\n",
        "PDB = \"\"\n",
        "\n",
        "files = sorted(os.listdir('/content/'))\n",
        "files = [f for f in files if f.endswith('.pdb')]\n",
        "\n",
        "if len(files) < 1:\n",
        "  raise ValueError('No PDB file found. Please upload your file before running this cell. Make sure it has a .pdb suffix.')\n",
        "elif len(files) > 1:\n",
        "  raise ValueError('Too many PDB files found. Please clear out any other PDBs before running this cell.')\n",
        "else:\n",
        "  pdb_file = os.path.join(\"/content/\", files[0])\n",
        "  PDB = files[0].removesuffix('.pdb')\n",
        "  print('Successfully uploaded PDB file %s' % (files[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "-bxta5Lvgheo"
      },
      "outputs": [],
      "source": [
        "#@markdown # **4. Run Model**\n",
        "\n",
        "#@markdown Stability model to use:\n",
        "Model = \"Single\" #@param [\"Epistatic\", \"Additive\", \"Single\"]\n",
        "\n",
        "#@markdown ##### Model descriptions:\n",
        "#@markdown * Single: Single mutation SSM sweep. Very fast and accurate.\n",
        "#@markdown * Additive: Naive double mutation SSM sweep. Ignores non-additive coupling. Very fast but less accurate than Epistatic model for picking stabilizing mutations.\n",
        "#@markdown * Epistatic: Full double mutation SSM sweep. Slower than Additive model, but more accurate for picking stabilizing mutations.\n",
        "\n",
        "#@markdown ---------------\n",
        "\n",
        "#@markdown Allow mutations to cysteine? (Not recommended)\n",
        "Include = False #@param {type: \"boolean\"}\n",
        "#@markdown Due to assay artifacts surrounding disulfide formation, model predictions for cysteine mutations may be overly favorable.\n",
        "\n",
        "#@markdown ---------------\n",
        "\n",
        "#@markdown Explicitly penalize disulfide breakage? (Recommended)\n",
        "Penalize = True #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown ThermoMPNN can usually detect disulfide breakage and penalize accordingly, but you may wish to explicitly forbid disulfide breakage to be safe. This option applies a flat penalty to make sure that breaking disulfides is always disfavored.\n",
        "\n",
        "#@markdown --------------\n",
        "\n",
        "#@markdown Batch size for model inference. (Recommended: 256 for Single/Additive models, 2048 for epistatic models)\n",
        "BatchSize = 256 #@param {type: \"integer\"}\n",
        "#@markdown If you hit a memory error, try lowering the BatchSize by factors of 2 to reduce memory usage.\n",
        "\n",
        "#@markdown --------------\n",
        "\n",
        "#@markdown Threshold for detecting stabilizing mutations. (Recommended: -1.0)\n",
        "Threshold = -1.0 #@param {type: \"number\"}\n",
        "#@markdown Only mutations with predicted ddG below this value will be kept for analysis. Higher thresholds will result in retaining more mutations.\n",
        "\n",
        "#@markdown --------------\n",
        "\n",
        "#@markdown Pairwise distance constraint for double mutants. (Recommended: 5.0)\n",
        "Distance = 5.0 #@param {type: \"number\"}\n",
        "#@markdown Only mutation pairs within this distance (in Angstrom) will be kept for analysis. Higher cutoffs will result in slower runtime and retaining more mutations.\n",
        "\n",
        "\n",
        "# use input_chain_list to grab correct protein chain\n",
        "chain_list = [c.strip() for c in Chains.strip().split(',')]\n",
        "if len(chain_list) == 1 and chain_list[0] == '':\n",
        "  chain_list = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FpkfycqVH0Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from thermompnn.datasets.dataset_utils import Mutation\n",
        "from thermompnn.datasets.v2_datasets import tied_featurize_mut\n",
        "from thermompnn.model.v2_model import _dist, batched_index_select\n",
        "from thermompnn.ssm_utils import (\n",
        "    distance_filter,\n",
        "    disulfide_penalty,\n",
        "    get_config,\n",
        "    get_dmat,\n",
        "    get_model,\n",
        "    load_pdb,\n",
        "    renumber_pdb,\n",
        ")\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def get_ssm_mutations_double(pdb, dthresh):\n",
        "    # make mutation list for SSM run\n",
        "    ALPHABET = \"ACDEFGHIKLMNPQRSTVWYX\"\n",
        "    MUT_POS, MUT_WT = [], []\n",
        "    for seq_pos in range(len(pdb[\"seq\"])):\n",
        "        wtAA = pdb[\"seq\"][seq_pos]\n",
        "        # check for missing residues\n",
        "        if wtAA != \"-\":\n",
        "            MUT_POS.append(seq_pos)\n",
        "            MUT_WT.append(wtAA)\n",
        "        else:\n",
        "            MUT_WT.append(\"-\")\n",
        "\n",
        "    # Use distance filter BEFORE data setup / inference for speedup\n",
        "    from thermompnn.ssm_utils import get_dmat\n",
        "\n",
        "    dmat = np.triu(get_dmat(pdb))  # [L, L]\n",
        "    mask = (dmat < dthresh) & (dmat > 0.0)\n",
        "    pos1, pos2 = np.where(mask)\n",
        "    pos_combos = [(p1, p2) for p1, p2 in zip(pos1, pos2)]\n",
        "    pos_combos = np.array(pos_combos)  # [combos, 2]\n",
        "    wtAA = np.zeros_like(pos_combos)\n",
        "    # fill in wtAA for each pos combo\n",
        "    for p_idx in range(pos_combos.shape[0]):\n",
        "        wtAA[p_idx, 0] = ALPHABET.index(MUT_WT[pos_combos[p_idx, 0]])\n",
        "        wtAA[p_idx, 1] = ALPHABET.index(MUT_WT[pos_combos[p_idx, 1]])\n",
        "\n",
        "    # make default mutAA bundle for broadcasting\n",
        "    one = np.arange(20).repeat(20)\n",
        "    two = np.tile(np.arange(20), 20)\n",
        "    mutAA = np.stack([one, two]).T  # [400, 2]\n",
        "    n_comb = pos_combos.shape[0]\n",
        "    mutAA = np.tile(mutAA, (n_comb, 1))\n",
        "\n",
        "    # the problem is 2nd wtAA/pos_combos and 2nd mutAA are correlated so they always show up together\n",
        "    # repeat these 20x20 times\n",
        "    wtAA = np.repeat(wtAA, 400, axis=0)\n",
        "    pos_combos = np.repeat(pos_combos, 400, axis=0)\n",
        "\n",
        "    # filter out self-mutations and single-mutations\n",
        "    mask = np.sum(mutAA == wtAA, -1).astype(bool)\n",
        "    pos_combos = pos_combos[~mask, :]\n",
        "    mutAA = mutAA[~mask, :]\n",
        "    wtAA = wtAA[~mask, :]\n",
        "\n",
        "    # filter out upper-triangle portions - if mutAA or pos is larger, it's already been checked\n",
        "    mask = pos_combos[:, 0] > pos_combos[:, 1]\n",
        "    pos_combos = pos_combos[~mask, :]\n",
        "    mutAA = mutAA[~mask, :]\n",
        "    wtAA = wtAA[~mask, :]\n",
        "\n",
        "    return torch.tensor(pos_combos), torch.tensor(wtAA), torch.tensor(mutAA)\n",
        "\n",
        "\n",
        "def run_double(\n",
        "    all_mpnn_hid, mpnn_embed, cfg, loader, batch_size, model, X, mask, mpnn_edges_raw\n",
        "):\n",
        "    \"\"\"Batched mutation processing using shared protein embeddings and only stability prediction module head\"\"\"\n",
        "    device = \"cuda\"\n",
        "    all_mpnn_hid = torch.cat(all_mpnn_hid[: cfg.model.num_final_layers], -1)\n",
        "    all_mpnn_hid = all_mpnn_hid.repeat(batch_size, 1, 1)\n",
        "    mpnn_embed = mpnn_embed.repeat(batch_size, 1, 1)\n",
        "    mpnn_edges_raw = mpnn_edges_raw.repeat(batch_size, 1, 1, 1)\n",
        "    # get edges between the two mutated residues\n",
        "    D_n, E_idx = _dist(X[:, :, 1, :], mask)\n",
        "    E_idx = E_idx.repeat(batch_size, 1, 1)\n",
        "\n",
        "    preds = []\n",
        "    for b in tqdm(loader):\n",
        "        pos, wtAA, mutAA = b\n",
        "        pos = pos.to(device)\n",
        "        wtAA = wtAA.to(device)\n",
        "        mutAA = mutAA.to(device)\n",
        "        mut_mutant_AAs = mutAA\n",
        "        mut_positions = pos\n",
        "        REAL_batch_size = mutAA.shape[0]\n",
        "\n",
        "        # get sequence embedding for mutant aa\n",
        "        mut_embed_list = []\n",
        "        for m in range(mut_mutant_AAs.shape[-1]):\n",
        "            mut_embed_list.append(model.prot_mpnn.W_s(mut_mutant_AAs[:, m]))\n",
        "        mut_embed = torch.cat(\n",
        "            [m.unsqueeze(-1) for m in mut_embed_list], -1\n",
        "        )  # shape: (Batch, Embed, N_muts)\n",
        "\n",
        "        n_mutations = [0, 1]\n",
        "        edges = []\n",
        "        for n_current in n_mutations:  # iterate over N-order mutations\n",
        "            # select the edges at the current mutated positions\n",
        "            if (\n",
        "                REAL_batch_size != mpnn_edges_raw.shape[0]\n",
        "            ):  # last batch will throw error if not corrected\n",
        "                mpnn_edges_raw = mpnn_edges_raw[:REAL_batch_size, ...]\n",
        "                E_idx = E_idx[:REAL_batch_size, ...]\n",
        "                all_mpnn_hid = all_mpnn_hid[:REAL_batch_size, ...]\n",
        "                mpnn_embed = mpnn_embed[:REAL_batch_size, ...]\n",
        "\n",
        "            mpnn_edges_tmp = torch.squeeze(\n",
        "                batched_index_select(\n",
        "                    mpnn_edges_raw, 1, mut_positions[:, n_current : n_current + 1]\n",
        "                ),\n",
        "                1,\n",
        "            )\n",
        "            E_idx_tmp = torch.squeeze(\n",
        "                batched_index_select(\n",
        "                    E_idx, 1, mut_positions[:, n_current : n_current + 1]\n",
        "                ),\n",
        "                1,\n",
        "            )\n",
        "\n",
        "            n_other = [a for a in n_mutations if a != n_current]\n",
        "            mp_other = mut_positions[:, n_other]  # [B, 1]\n",
        "            # E_idx_tmp [B, K]\n",
        "            mp_other = mp_other[..., None].repeat(\n",
        "                1, 1, E_idx_tmp.shape[-1]\n",
        "            )  # [B, 1, 48]\n",
        "            idx = torch.where(\n",
        "                E_idx_tmp[:, None, :] == mp_other\n",
        "            )  # get indices where the neighbor list matches the mutations we want\n",
        "            a, b, c = idx\n",
        "            # start w/empty edges and fill in as you go, then set remaining edges to 0\n",
        "            edge = torch.full(\n",
        "                [REAL_batch_size, mpnn_edges_tmp.shape[-1]],\n",
        "                torch.nan,\n",
        "                device=E_idx.device,\n",
        "            )  # [B, 128]\n",
        "            # idx is (a, b, c) tuple of tensors\n",
        "            # a has indices of batch members; b is all 0s; c has indices of actual neighbors for edge grabbing\n",
        "            edge[a, :] = mpnn_edges_tmp[a, c, :]\n",
        "            edge = torch.nan_to_num(edge, nan=0)\n",
        "            edges.append(edge)\n",
        "\n",
        "        mpnn_edges = torch.stack(\n",
        "            edges, dim=-1\n",
        "        )  # this should get two edges per set of doubles (one for each)\n",
        "\n",
        "        # gather final representation from seq and structure embeddings\n",
        "        final_embed = []\n",
        "        for i in range(mut_mutant_AAs.shape[-1]):\n",
        "            # gather embedding for a specific position\n",
        "            current_positions = mut_positions[:, i : i + 1]  # [B, 1]\n",
        "            g_struct_embed = torch.gather(\n",
        "                all_mpnn_hid,\n",
        "                1,\n",
        "                current_positions.unsqueeze(-1).expand(\n",
        "                    current_positions.size(0),\n",
        "                    current_positions.size(1),\n",
        "                    all_mpnn_hid.size(2),\n",
        "                ),\n",
        "            )\n",
        "            g_struct_embed = torch.squeeze(g_struct_embed, 1)  # [B, E * nfl]\n",
        "            # add specific mutant embedding to gathered embed based on which mutation is being gathered\n",
        "            g_seq_embed = torch.gather(\n",
        "                mpnn_embed,\n",
        "                1,\n",
        "                current_positions.unsqueeze(-1).expand(\n",
        "                    current_positions.size(0),\n",
        "                    current_positions.size(1),\n",
        "                    mpnn_embed.size(2),\n",
        "                ),\n",
        "            )\n",
        "            g_seq_embed = torch.squeeze(g_seq_embed, 1)  # [B, E]\n",
        "            # if mut embed enabled, subtract it from the wt embed directly to keep dims low\n",
        "            if cfg.model.mutant_embedding:\n",
        "                if REAL_batch_size != mut_embed.shape[0]:\n",
        "                    mut_embed = mut_embed[:REAL_batch_size, ...]\n",
        "                g_seq_embed = g_seq_embed - mut_embed[:, :, i]  # [B, E]\n",
        "            g_embed = torch.cat([g_struct_embed, g_seq_embed], -1)  # [B, E * (nfl + 1)]\n",
        "\n",
        "            # if edges enabled, concatenate them onto the end of the embedding\n",
        "            if cfg.model.edges:\n",
        "                g_edge_embed = mpnn_edges[:, :, i]\n",
        "                g_embed = torch.cat([g_embed, g_edge_embed], -1)  # [B, E * (nfl + 2)]\n",
        "            final_embed.append(\n",
        "                g_embed\n",
        "            )  # list with length N_mutations - used to make permutations\n",
        "        final_embed = torch.stack(final_embed, dim=0)  # [2, B, E x (nfl + 1)]\n",
        "\n",
        "        # do initial dim reduction\n",
        "        final_embed = model.light_attention(final_embed)  # [2, B, E]\n",
        "\n",
        "        # if batch is only single mutations, pad it out with a \"zero\" mutation\n",
        "        if final_embed.shape[0] == 1:\n",
        "            zero_embed = torch.zeros(\n",
        "                final_embed.shape, dtype=torch.float32, device=E_idx.device\n",
        "            )\n",
        "            final_embed = torch.cat([final_embed, zero_embed], dim=0)\n",
        "\n",
        "        # make two copies, one with AB order and other with BA order of mutation\n",
        "        embedAB = torch.cat((final_embed[0, :, :], final_embed[1, :, :]), dim=-1)\n",
        "        embedBA = torch.cat((final_embed[1, :, :], final_embed[0, :, :]), dim=-1)\n",
        "\n",
        "        ddG_A = model.ddg_out(embedAB)  # [B, 1]\n",
        "        ddG_B = model.ddg_out(embedBA)  # [B, 1]\n",
        "\n",
        "        ddg = (ddG_A + ddG_B) / 2.0\n",
        "        preds += list(torch.squeeze(ddg, dim=-1).detach().cpu().numpy())\n",
        "    return np.squeeze(preds)\n",
        "\n",
        "\n",
        "class SSMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, POS, WTAA, MUTAA):\n",
        "        self.POS = POS\n",
        "        self.WTAA = WTAA\n",
        "        self.MUTAA = MUTAA\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.POS.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.POS[index, :], self.WTAA[index, :], self.MUTAA[index, :]\n",
        "\n",
        "\n",
        "def run_single_ssm(pdb, cfg, model):\n",
        "    \"\"\"Runs single-mutant SSM sweep with ThermoMPNN v2\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "    stime = time.time()\n",
        "\n",
        "    # placeholder mutation to keep featurization from throwing error\n",
        "    pdb[\"mutation\"] = Mutation([0], [\"A\"], [\"A\"], [0.0], \"\")\n",
        "\n",
        "    # featurize input\n",
        "    device = \"cuda\"\n",
        "    batch = tied_featurize_mut([pdb])\n",
        "    (\n",
        "        X,\n",
        "        S,\n",
        "        mask,\n",
        "        lengths,\n",
        "        chain_M,\n",
        "        chain_encoding_all,\n",
        "        residue_idx,\n",
        "        mut_positions,\n",
        "        mut_wildtype_AAs,\n",
        "        mut_mutant_AAs,\n",
        "        mut_ddGs,\n",
        "        atom_mask,\n",
        "    ) = batch\n",
        "\n",
        "    X = X.to(device)\n",
        "    S = S.to(device)\n",
        "    mask = mask.to(device)\n",
        "    lengths = torch.Tensor(lengths).to(device)\n",
        "    chain_M = chain_M.to(device)\n",
        "    chain_encoding_all = chain_encoding_all.to(device)\n",
        "    residue_idx = residue_idx.to(device)\n",
        "    mut_ddGs = mut_ddGs.to(device)\n",
        "\n",
        "    # do single pass through thermompnn\n",
        "    X = torch.nan_to_num(X, nan=0.0)\n",
        "    all_mpnn_hid, mpnn_embed, _, mpnn_edges = model.prot_mpnn(\n",
        "        X, S, mask, chain_M, residue_idx, chain_encoding_all\n",
        "    )\n",
        "\n",
        "    all_mpnn_hid = torch.cat(all_mpnn_hid[: cfg.model.num_final_layers], -1)\n",
        "    all_mpnn_hid = torch.squeeze(torch.cat([all_mpnn_hid, mpnn_embed], -1), 0)  # [L, E]\n",
        "\n",
        "    all_mpnn_hid = model.light_attention(torch.unsqueeze(all_mpnn_hid, -1))\n",
        "\n",
        "    ddg = model.ddg_out(all_mpnn_hid)  # [L, 21]\n",
        "\n",
        "    # subtract wildtype ddgs to normalize\n",
        "    S = torch.squeeze(S)  # [L, ]\n",
        "\n",
        "    wt_ddg = batched_index_select(ddg, dim=-1, index=S)  # [L, 1]\n",
        "    ddg = ddg - wt_ddg.expand(-1, 21)  # [L, 21]\n",
        "    etime = time.time()\n",
        "    elapsed = etime - stime\n",
        "    length = ddg.shape[0]\n",
        "    print(\n",
        "        f\"ThermoMPNN single mutant predictions generated for protein of length {length} in {round(elapsed, 2)} seconds.\"\n",
        "    )\n",
        "    return ddg, S\n",
        "\n",
        "def expand_additive(ddg):\n",
        "    \"\"\"Uses torch broadcasting to add all possible single mutants to each other in a vectorized operation.\"\"\"\n",
        "    # ddg [L, 21]\n",
        "    dims = ddg.shape\n",
        "    ddgA = ddg.reshape(dims[0], dims[1], 1, 1)  # [L, 21, 1, 1]\n",
        "    ddgB = ddg.reshape(1, 1, dims[0], dims[1])  # [1, 1, L, 21]\n",
        "    ddg = ddgA + ddgB  # L, 21, L, 21\n",
        "\n",
        "    # mask out diagonal representing two mutations at the same position - this is invalid\n",
        "    for i in range(dims[0]):\n",
        "        ddg[i, :, i, :] = torch.nan\n",
        "\n",
        "    return ddg\n",
        "\n",
        "\n",
        "def format_output_single(ddg, S, threshold=-0.5):\n",
        "    \"\"\"Converts raw SSM predictions into nice format for analysis\"\"\"\n",
        "    ALPHABET = \"ACDEFGHIKLMNPQRSTVWYX\"\n",
        "    ddg = ddg.cpu().detach().numpy()\n",
        "    ddg = ddg[:, :20]\n",
        "\n",
        "    keep_L, keep_AA = np.where(ddg <= threshold)\n",
        "    ddg = ddg[ddg <= threshold]  # [N, ]\n",
        "\n",
        "    mutlist = []\n",
        "    for L_idx, AA_idx in tqdm(zip(keep_L, keep_AA)):\n",
        "        wtAA = ALPHABET[S[L_idx]]\n",
        "        mutAA = ALPHABET[AA_idx]\n",
        "        mutlist.append(wtAA + str(L_idx + 1) + mutAA)\n",
        "\n",
        "    return ddg, mutlist\n",
        "\n",
        "\n",
        "def format_output_double(ddg, S, threshold, pdb, distance):\n",
        "    \"\"\"Converts raw SSM predictions into nice format for analysis\"\"\"\n",
        "    stime = time.time()\n",
        "    ALPHABET = \"ACDEFGHIKLMNPQRSTVWYX\"\n",
        "    ddg = ddg.cpu().detach().numpy()  # [L, 21]\n",
        "    L, AA = ddg.shape\n",
        "\n",
        "    ddg = expand_additive(ddg)  # [L, 21, L, 21]\n",
        "    ddg = ddg[:, :20, :, :20]  # drop X predictions\n",
        "\n",
        "    # Pre-mask matrix with distance constraints for speedup\n",
        "    dmat = get_dmat(pdb)\n",
        "    assert ddg.shape[0] == dmat.shape[0]\n",
        "    valid_mask = (\n",
        "        (ddg <= threshold)\n",
        "        * (dmat < distance)[:, None, :, None]\n",
        "        * (dmat != 0.0)[:, None, :, None]\n",
        "    )\n",
        "    p1s, a1s, p2s, a2s = np.where(valid_mask)\n",
        "\n",
        "    cond = p1s < p2s  # filter to keep only upper triangle\n",
        "    p1s, a1s, p2s, a2s = p1s[cond], a1s[cond], p2s[cond], a2s[cond]\n",
        "    wt_seq = [ALPHABET[S[ppp]] for ppp in np.arange(L)]\n",
        "\n",
        "    mutlist, ddglist = [], []\n",
        "    for p1, a1, p2, a2 in tqdm(zip(p1s, a1s, p2s, a2s)):\n",
        "        wt1, wt2 = wt_seq[p1], wt_seq[p2]\n",
        "        mut1, mut2 = ALPHABET[a1], ALPHABET[a2]\n",
        "\n",
        "        if (wt1 != mut1) and (wt2 != mut2):  # drop self-mutations\n",
        "            mutation = f\"{wt1}{p1 + 1}{mut1}:{wt2}{p2 + 1}{mut2}\"\n",
        "            mutlist.append(mutation)\n",
        "            ddglist.append(ddg[p1, a1, p2, a2])\n",
        "\n",
        "    etime = time.time()\n",
        "    elapsed = etime - stime\n",
        "    print(\n",
        "        f\"ThermoMPNN double mutant additive model predictions calculated in {round(elapsed, 2)} seconds.\"\n",
        "    )\n",
        "    return ddglist, mutlist\n",
        "\n",
        "\n",
        "def format_output_epistatic(ddg, S, pos, wtAA, mutAA, threshold=-0.5):\n",
        "    \"Converts raw SSM predictions into nice format for analysis.\"\n",
        "    stime = time.time()\n",
        "    ALPHABET = \"ACDEFGHIKLMNPQRSTVWYX\"\n",
        "    S = torch.squeeze(S)\n",
        "\n",
        "    # filter out ddgs that miss the threshold\n",
        "    mask = ddg <= threshold\n",
        "    ddg = ddg[mask]\n",
        "    wtAA = wtAA[mask, :]\n",
        "    mutAA = mutAA[mask, :]\n",
        "    pos = pos[mask, :]\n",
        "    mut_list = []\n",
        "    # a bunch of repeats in here?!\n",
        "    for b in tqdm(range(ddg.shape[0])):\n",
        "        w1 = ALPHABET[wtAA[b, 0]]\n",
        "        w2 = ALPHABET[wtAA[b, 1]]\n",
        "        m1 = ALPHABET[mutAA[b, 0]]\n",
        "        m2 = ALPHABET[mutAA[b, 1]]\n",
        "        mut_name = f\"{w1}{pos[b, 0] + 1}{m1}:{w2}{pos[b, 1] + 1}{m2}\"\n",
        "        mut_list.append(mut_name)\n",
        "    etime = time.time()\n",
        "    elapsed = etime - stime\n",
        "    print(\n",
        "        f\"ThermoMPNN double mutant epistatic model predictions sorted and filtered in {round(elapsed, 2)} seconds.\"\n",
        "    )\n",
        "    return ddg, mut_list\n",
        "\n",
        "\n",
        "def run_epistatic_ssm(pdb, cfg, model, distance, threshold, batch_size):\n",
        "    \"\"\"Run epistatic model on double mutations\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "    stime = time.time()\n",
        "\n",
        "    # placeholder mutation to keep featurization from throwing error\n",
        "    pdb[\"mutation\"] = Mutation([0], [\"A\"], [\"A\"], [0.0], \"\")\n",
        "\n",
        "    # featurize input\n",
        "    device = \"cuda\"\n",
        "    batch = tied_featurize_mut([pdb])\n",
        "    (\n",
        "        X,\n",
        "        S,\n",
        "        mask,\n",
        "        lengths,\n",
        "        chain_M,\n",
        "        chain_encoding_all,\n",
        "        residue_idx,\n",
        "        mut_positions,\n",
        "        mut_wildtype_AAs,\n",
        "        mut_mutant_AAs,\n",
        "        mut_ddGs,\n",
        "        atom_mask,\n",
        "    ) = batch\n",
        "\n",
        "    X = X.to(device)\n",
        "    S = S.to(device)\n",
        "    mask = mask.to(device)\n",
        "    lengths = torch.Tensor(lengths).to(device)\n",
        "    chain_M = chain_M.to(device)\n",
        "    chain_encoding_all = chain_encoding_all.to(device)\n",
        "    residue_idx = residue_idx.to(device)\n",
        "    mut_ddGs = mut_ddGs.to(device)\n",
        "\n",
        "    # do single pass through thermompnn\n",
        "    X = torch.nan_to_num(X, nan=0.0)\n",
        "    all_mpnn_hid, mpnn_embed, _, mpnn_edges = model.prot_mpnn(\n",
        "        X, S, mask, chain_M, residue_idx, chain_encoding_all\n",
        "    )\n",
        "\n",
        "    # grab double mutation inputs\n",
        "    MUT_POS, MUT_WT_AA, MUT_MUT_AA = get_ssm_mutations_double(pdb, distance)\n",
        "    dataset = SSMDataset(MUT_POS, MUT_WT_AA, MUT_MUT_AA)\n",
        "    loader = DataLoader(dataset, shuffle=False, batch_size=batch_size, num_workers=8)\n",
        "\n",
        "    preds = run_double(\n",
        "        all_mpnn_hid, mpnn_embed, cfg, loader, batch_size, model, X, mask, mpnn_edges\n",
        "    )\n",
        "    ddg, mutations = format_output_epistatic(\n",
        "        preds, S, MUT_POS, MUT_WT_AA, MUT_MUT_AA, threshold\n",
        "    )\n",
        "\n",
        "    etime = time.time()\n",
        "    elapsed = etime - stime\n",
        "    print(\n",
        "        f\"ThermoMPNN double mutant epistatic model predictions generated in {round(elapsed, 2)} seconds.\"\n",
        "    )\n",
        "    return ddg, mutations\n",
        "\n",
        "\n",
        "def check_df_size(size):\n",
        "    if size == 0:\n",
        "        raise ValueError(\"No valid mutations passed your distance and ddG filters. Please increase one or both of these parameters and try again.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6aWvCK-FCn5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Add this for better error messages\n",
        "\n",
        "def run_single_ssm_synchronized(pdb_data, cfg, model, chains=['A', 'B', 'C']):\n",
        "    \"\"\"Run single SSM with synchronized mutations across chains\"\"\"\n",
        "    # Set model to evaluation mode and move to appropriate device\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    print(f\"Model moved to {device}\")\n",
        "\n",
        "    stime = time.time()\n",
        "\n",
        "    # Placeholder mutation to prevent featurization errors\n",
        "    pdb_data[\"mutation\"] = Mutation(position=0, wildtype=\"A\", mutation=\"A\", ddG=0.0, pdb=pdb_data['name'])\n",
        "\n",
        "    # Featurize input\n",
        "    batch = tied_featurize_mut([pdb_data])\n",
        "    if batch is None:\n",
        "        raise ValueError(\"Batch featurization failed\")\n",
        "\n",
        "    (X, S, mask, lengths, chain_M, chain_encoding_all, residue_idx,\n",
        "     mut_positions, mut_wildtype_AAs, mut_mutant_AAs, mut_ddGs, atom_mask) = batch\n",
        "\n",
        "    # Debugging outputs\n",
        "    print(f\"X shape: {X.shape}\")\n",
        "    print(f\"S shape: {S.shape}\")\n",
        "    print(f\"mask shape: {mask.shape}\")\n",
        "\n",
        "    try:\n",
        "        # Move tensors to device\n",
        "        X = X.to(device)\n",
        "        S = S.to(device)\n",
        "        mask = mask.to(device)\n",
        "        lengths = torch.Tensor(lengths).to(device)\n",
        "        chain_M = chain_M.to(device)\n",
        "        chain_encoding_all = chain_encoding_all.to(device)\n",
        "        residue_idx = residue_idx.to(device)\n",
        "        mut_ddGs = mut_ddGs.to(device)\n",
        "\n",
        "        # Forward pass through prot_mpnn\n",
        "        X = torch.nan_to_num(X, nan=0.0)\n",
        "        with torch.no_grad():\n",
        "            print(\"Starting prot_mpnn forward pass\")\n",
        "            all_mpnn_hid, mpnn_embed, _, mpnn_edges = model.prot_mpnn(\n",
        "                X, S, mask, chain_M, residue_idx, chain_encoding_all\n",
        "            )\n",
        "            print(\"prot_mpnn forward pass complete\")\n",
        "\n",
        "            # Concatenate hidden layers\n",
        "            all_mpnn_hid = torch.cat(all_mpnn_hid[: cfg.model.num_final_layers], dim=-1)\n",
        "            all_mpnn_hid = torch.cat([all_mpnn_hid, mpnn_embed], dim=-1).squeeze(0)\n",
        "            all_mpnn_hid = model.light_attention(all_mpnn_hid.unsqueeze(-1))\n",
        "            ddg = model.ddg_out(all_mpnn_hid)\n",
        "\n",
        "        # Normalize predictions\n",
        "        S = torch.squeeze(S)\n",
        "        wt_ddg = batched_index_select(ddg, dim=-1, index=S)\n",
        "        ddg = ddg - wt_ddg.expand(-1, 21)\n",
        "\n",
        "        # Determine chain length and number of chains\n",
        "        chain_length = len(pdb_data[f'seq_chain_{chains[0]}'])\n",
        "        n_chains = len(chains)\n",
        "\n",
        "        print(f\"ddg shape: {ddg.shape}\")\n",
        "        print(f\"chain_length: {chain_length}, n_chains: {n_chains}\")\n",
        "\n",
        "        # Reshape ddg to [n_chains, chain_length, 21]\n",
        "        ddg = ddg.view(n_chains, chain_length, 21)\n",
        "\n",
        "        # Sum ddG across chains to represent the total stability change\n",
        "        ddg_total = ddg.sum(dim=0)  # [chain_length, 21]\n",
        "\n",
        "        # Create mutation list with separate Mutation instances per chain\n",
        "        mutations_list = []\n",
        "        ALPHABET = \"ACDEFGHIKLMNPQRSTVWY-\"\n",
        "\n",
        "        for pos in range(chain_length):\n",
        "            wt_aa = pdb_data[f'seq_chain_{chains[0]}'][pos]\n",
        "            if wt_aa == '-':\n",
        "                continue\n",
        "\n",
        "            for mut_idx, mut_aa in enumerate(ALPHABET):\n",
        "                if mut_aa != wt_aa and mut_aa != '-':\n",
        "                    for chain in chains:\n",
        "                        mut = Mutation(\n",
        "                            position=pos,\n",
        "                            wildtype=wt_aa,\n",
        "                            mutation=mut_aa,\n",
        "                            ddG=ddg_total[pos, mut_idx].item(),\n",
        "                            pdb=pdb_data['name']\n",
        "                        )\n",
        "                        mutations_list.append(mut)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    etime = time.time()\n",
        "    print(f\"Predictions generated in {round(etime-stime, 2)} seconds.\")\n",
        "\n",
        "    return ddg_total, mutations_list\n",
        "\n",
        "\n",
        "# Format output\n",
        "def format_synchronized_output(ddg, mutations, threshold=-1.0):\n",
        "    \"\"\"Format the synchronized mutations output\"\"\"\n",
        "    formatted_muts = []\n",
        "    for mut in mutations:\n",
        "        if mut.ddG < threshold:\n",
        "            mut_str = ':'.join([f\"{w}{p+1}{m}\" for w, p, m in\n",
        "                              zip(mut.wildtype, mut.position, mut.mutation)])\n",
        "            formatted_muts.append({\n",
        "                'ddG (kcal/mol)': mut.ddG,\n",
        "                'Mutation': mut_str\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(formatted_muts)\n",
        "    return df.sort_values('ddG (kcal/mol)')"
      ],
      "metadata": {
        "id": "CnBgrN8kH3hj",
        "outputId": "75205f35-b4a9-4c47-9cba-eb57e9613c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in main execution: CUDA error: device-side assert triggered\n",
            "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
            "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
            "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-75dda82179e8>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthermompnn_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/ThermoMPNN-D'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# Run predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ThermoMPNN-D/thermompnn/ssm_utils.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(mode, config)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"additive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_weights/ThermoMPNN-ens1.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTransferModelPLv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"epistatic\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/model_helpers.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;34m\" Please call it on the class type and make sure the return value is used.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 )\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \"\"\"\n\u001b[0;32m-> 1581\u001b[0;31m         loaded = _load_from_checkpoint(\n\u001b[0m\u001b[1;32m   1582\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_default_map_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpl_legacy_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# convert legacy checkpoints to the new format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_fabric/utilities/cloud_io.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location, weights_only)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         return torch.load(\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m                 return _load(\n\u001b[0m\u001b[1;32m   1361\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/pickle.py\u001b[0m in \u001b[0;36mload_binpersid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_binpersid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBINPERSID\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_binpersid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m             typed_storage = load_tensor(\n\u001b[0m\u001b[1;32m   1813\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m             \u001b[0m_internal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1695\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \"\"\"\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     ) -> Union[_StorageBase, TypedStorage]:\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     87\u001b[0m             ), f\"sparse storage is not supported for {device.type.upper()} tensors\"\n\u001b[1;32m     88\u001b[0m             \u001b[0muntyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0muntyped_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0muntyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # **Run SSM Inference**\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from thermompnn.ssm_utils import (\n",
        "    distance_filter,\n",
        "    disulfide_penalty,\n",
        "    get_config,\n",
        "    get_dmat,\n",
        "    get_model,\n",
        "    load_pdb,\n",
        "    renumber_pdb,\n",
        ")\n",
        "# from v2_ssm import (\n",
        "#     run_single_ssm,\n",
        "#     run_epistatic_ssm,\n",
        "#     format_output_single,\n",
        "#     format_output_double,\n",
        "#     check_df_size,\n",
        "#     run_single_ssm_synchronized\n",
        "# )\n",
        "\n",
        "# ------------ MAIN INFERENCE ROUTINE -------------- #\n",
        "\n",
        "mode = Model.lower()\n",
        "pdb = pdb_file\n",
        "chains = chain_list\n",
        "threshold = Threshold\n",
        "distance = Distance\n",
        "batch_size = BatchSize\n",
        "ss_penalty = Penalize\n",
        "\n",
        "cfg = get_config(mode)\n",
        "cfg.platform.thermompnn_dir = '/content/ThermoMPNN-D'\n",
        "model = get_model(mode, cfg)\n",
        "pdb_data = load_pdb(pdb, chains)\n",
        "pdbname = os.path.basename(pdb)\n",
        "print(f\"Loaded PDB {pdbname}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "I3kIwKs0__TP",
        "outputId": "87e18bd5-eaa3-4bf2-9e7b-1888af2e58ff"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-5fd16fbd9594>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthermompnn_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/ThermoMPNN-D'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mpdb_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mpdbname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ThermoMPNN-D/thermompnn/ssm_utils.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(mode, config)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"additive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_weights/ThermoMPNN-ens1.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTransferModelPLv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"epistatic\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/model_helpers.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;34m\" Please call it on the class type and make sure the return value is used.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 )\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \"\"\"\n\u001b[0;32m-> 1581\u001b[0;31m         loaded = _load_from_checkpoint(\n\u001b[0m\u001b[1;32m   1582\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_default_map_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpl_legacy_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# convert legacy checkpoints to the new format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning_fabric/utilities/cloud_io.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location, weights_only)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         return torch.load(\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m                 return _load(\n\u001b[0m\u001b[1;32m   1361\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/pickle.py\u001b[0m in \u001b[0;36mload_binpersid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_binpersid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBINPERSID\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_binpersid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1810\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m             typed_storage = load_tensor(\n\u001b[0m\u001b[1;32m   1813\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m             \u001b[0m_internal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1695\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \"\"\"\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     ) -> Union[_StorageBase, TypedStorage]:\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     87\u001b[0m             ), f\"sparse storage is not supported for {device.type.upper()} tensors\"\n\u001b[1;32m     88\u001b[0m             \u001b[0muntyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0muntyped_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0muntyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdb_data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhuPjxwiD3FY",
        "outputId": "e6cf663f-9de5-4d35-b065-3dac0eea6090"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['resn_list_A', 'seq_chain_A', 'coords_chain_A', 'resn_list_B', 'seq_chain_B', 'coords_chain_B', 'resn_list_C', 'seq_chain_C', 'coords_chain_C', 'name', 'num_of_chains', 'seq', 'mutation'])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdb_data['seq_chain_C']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "2doSELNUEOxA",
        "outputId": "9a6bc34b-4bbe-41a0-9da9-a096e72e089a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIH----------DNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGV---------------------NCTFEYVS-------------FKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALH-----------------AAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSK--GNYNYLYR-------KPFERDI--------------------YFPLQSYGFQPTN-VGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEV--------------------NVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQT------------SQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVT--------------------------KFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDPPEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDS'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddgs, mutations = run_single_ssm_synchronized(pdb_data, cfg, model, chains=['A', 'B', 'C'])\n",
        "df = format_synchronized_output(ddgs, mutations, threshold=-1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "8Lg_meKEAG5L",
        "outputId": "88225f93-d7eb-44c7-efb3-6f2e880f3ee4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-7df5bddb26cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_single_ssm_synchronized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdb_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_synchronized_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-ccfdf1a078f5>\u001b[0m in \u001b[0;36mrun_single_ssm_synchronized\u001b[0;34m(pdb_data, cfg, model, chains)\u001b[0m\n\u001b[1;32m    325\u001b[0m     ) = batch\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qisnoLI-iddh",
        "outputId": "c38dfbe7-6fdb-4c08-b729-3a23f2475dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model %s /content/ThermoMPNN-D/vanilla_model_weights/v_48_020.pt\n",
            "setting ProteinMPNN dropout: 0.0\n",
            "MLP HIDDEN SIZES: [384, 64, 32, 21]\n",
            "Loaded PDB 1vii.pdb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/ThermoMPNN-D/thermompnn/model/modules.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer. For large datasets, this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ThermoMPNN single mutant predictions generated for protein of length 36 in 0.92 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2it [00:00, 4038.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ThermoMPNN predictions renumbered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "if (mode == \"single\") or (mode == \"additive\"):\n",
        "  ddg, S = run_single_ssm(pdb_data, cfg, model)\n",
        "\n",
        "  if mode == \"single\":\n",
        "    ddg, mutations = format_output_single(ddg, S, threshold)\n",
        "  else:\n",
        "    ddg, mutations = format_output_double(\n",
        "      ddg, S, threshold, pdb_data, distance\n",
        "    )\n",
        "\n",
        "elif mode == \"epistatic\":\n",
        "  ddg, mutations = run_epistatic_ssm(\n",
        "    pdb_data, cfg, model, distance, threshold, batch_size\n",
        "  )\n",
        "\n",
        "else:\n",
        "  raise ValueError(\"Invalid mode selected!\")\n",
        "\n",
        "df = pd.DataFrame({\"ddG (kcal/mol)\": ddg, \"Mutation\": mutations})\n",
        "\n",
        "check_df_size(df.shape[0])\n",
        "\n",
        "if mode != \"single\":\n",
        "  df = distance_filter(df, pdb_data, distance)\n",
        "\n",
        "if ss_penalty:\n",
        "  df = disulfide_penalty(df, pdb_data, mode)\n",
        "\n",
        "if not Include:\n",
        "  df = drop_cysteines(df, mode)\n",
        "\n",
        "df = df.dropna(subset=[\"ddG (kcal/mol)\"])\n",
        "if threshold <= -0.0:\n",
        "  df = df.sort_values(by=[\"ddG (kcal/mol)\"])\n",
        "\n",
        "if mode != \"single\":  # sort to have neat output order\n",
        "  df[[\"mut1\", \"mut2\"]] = df[\"Mutation\"].str.split(\":\", n=2, expand=True)\n",
        "  df[\"pos1\"] = df[\"mut1\"].str[1:-1].astype(int) + 1\n",
        "  df[\"pos2\"] = df[\"mut2\"].str[1:-1].astype(int) + 1\n",
        "\n",
        "  df = df.sort_values(by=[\"pos1\", \"pos2\"])\n",
        "  df = df[[\"ddG (kcal/mol)\", \"Mutation\", \"CA-CA Distance\"]].reset_index(drop=True)\n",
        "\n",
        "check_df_size(df.shape[0])\n",
        "\n",
        "try:\n",
        "  df = renumber_pdb(df, pdb_data, mode)\n",
        "\n",
        "except (KeyError, IndexError):\n",
        "  print(\n",
        "    \"PDB renumbering failed (sorry!) You can still use the raw position data. Or, you can renumber your PDB, fill any weird gaps, and try again.\"\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "rpdMGazKO0g1",
        "outputId": "4399ec01-a160-490d-9fab-e40069210d94",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ddG (kcal/mol)</th>\n",
              "      <th>Mutation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.531715</td>\n",
              "      <td>KA70W</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.311499</td>\n",
              "      <td>KA70Y</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/e523c247d1e24a05/data_table.js\";\n\n      const table = window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': -1.5317153930664062,\n            'f': \"-1.5317153930664062\",\n        },\n\"KA70W\"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n{\n            'v': -1.3114986419677734,\n            'f': \"-1.3114986419677734\",\n        },\n\"KA70Y\"]],\n        columns: [[\"number\", \"index\"], [\"number\", \"ddG (kcal/mol)\"], [\"string\", \"Mutation\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 10,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n\n      function appendQuickchartButton(parentElement) {\n        let quickchartButtonContainerElement = document.createElement('div');\n        quickchartButtonContainerElement.innerHTML = `\n<div id=\"df-0c483008-0b75-462f-bd4c-7a1aaa147bdc\">\n  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c483008-0b75-462f-bd4c-7a1aaa147bdc')\"\n            title=\"Suggest charts\"\n            style=\"display:none;\">\n    \n<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n     width=\"24px\">\n    <g>\n        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n    </g>\n</svg>\n  </button>\n  \n<style>\n  .colab-df-quickchart {\n      --bg-color: #E8F0FE;\n      --fill-color: #1967D2;\n      --hover-bg-color: #E2EBFA;\n      --hover-fill-color: #174EA6;\n      --disabled-fill-color: #AAA;\n      --disabled-bg-color: #DDD;\n  }\n\n  [theme=dark] .colab-df-quickchart {\n      --bg-color: #3B4455;\n      --fill-color: #D2E3FC;\n      --hover-bg-color: #434B5C;\n      --hover-fill-color: #FFFFFF;\n      --disabled-bg-color: #3B4455;\n      --disabled-fill-color: #666;\n  }\n\n  .colab-df-quickchart {\n    background-color: var(--bg-color);\n    border: none;\n    border-radius: 50%;\n    cursor: pointer;\n    display: none;\n    fill: var(--fill-color);\n    height: 32px;\n    padding: 0;\n    width: 32px;\n  }\n\n  .colab-df-quickchart:hover {\n    background-color: var(--hover-bg-color);\n    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n    fill: var(--button-hover-fill-color);\n  }\n\n  .colab-df-quickchart-complete:disabled,\n  .colab-df-quickchart-complete:disabled:hover {\n    background-color: var(--disabled-bg-color);\n    fill: var(--disabled-fill-color);\n    box-shadow: none;\n  }\n\n  .colab-df-spinner {\n    border: 2px solid var(--fill-color);\n    border-color: transparent;\n    border-bottom-color: var(--fill-color);\n    animation:\n      spin 1s steps(1) infinite;\n  }\n\n  @keyframes spin {\n    0% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n      border-left-color: var(--fill-color);\n    }\n    20% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    30% {\n      border-color: transparent;\n      border-left-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n      border-right-color: var(--fill-color);\n    }\n    40% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-top-color: var(--fill-color);\n    }\n    60% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n    }\n    80% {\n      border-color: transparent;\n      border-right-color: var(--fill-color);\n      border-bottom-color: var(--fill-color);\n    }\n    90% {\n      border-color: transparent;\n      border-bottom-color: var(--fill-color);\n    }\n  }\n</style>\n\n  <script>\n    async function quickchart(key) {\n      const quickchartButtonEl =\n        document.querySelector('#' + key + ' button');\n      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n      quickchartButtonEl.classList.add('colab-df-spinner');\n      try {\n        const charts = await google.colab.kernel.invokeFunction(\n            'suggestCharts', [key], {});\n      } catch (error) {\n        console.error('Error during call to suggestCharts:', error);\n      }\n      quickchartButtonEl.classList.remove('colab-df-spinner');\n      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n    }\n    (() => {\n      let quickchartButtonEl =\n        document.querySelector('#df-0c483008-0b75-462f-bd4c-7a1aaa147bdc button');\n      quickchartButtonEl.style.display =\n        google.colab.kernel.accessAllowed ? 'block' : 'none';\n    })();\n  </script>\n</div>`;\n        parentElement.appendChild(quickchartButtonContainerElement);\n      }\n\n      appendQuickchartButton(table);\n    ",
            "text/plain": [
              "<google.colab.data_table.DataTable object>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#@title **Visualize data in an interactive table**\n",
        "from google.colab import data_table\n",
        "\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(df, include_index=True, num_rows_per_page=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "OX0cjtdNPJoA",
        "outputId": "602e14d1-21bd-4cfb-e036-80c9f336db55",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7ed7568f-45e9-4786-87b4-38eccccade70\", \"example.csv\", 13617)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title # **Save Output as CSV**\n",
        "\n",
        "# ---------- Collect output into DF and save as CSV ---------- #\n",
        "from google.colab import files\n",
        "\n",
        "#@markdown Specify prefix for file saving (e.g., MyProtein). Leave blank to use input PDB code.\n",
        "PREFIX = \"MyProtein\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown If you wish to retrieve your files manually, you may do so in the **Files** tab in the leftmost toolbar.\n",
        "\n",
        "#@markdown NOTE: Make sure you click \"Allow\" if your browser asks to permit downloads at this step.\n",
        "\n",
        "#@markdown -------------\n",
        "\n",
        "#@markdown Save verbose output? (Recommended: True)\n",
        "VERBOSE = True #@param {type: \"boolean\"}\n",
        "#@markdown If enabled, more detailed mutation information will be saved.\n",
        "\n",
        "df['ddG (kcal/mol)'] = df['ddG (kcal/mol)'].round(4)\n",
        "\n",
        "if len(PREFIX) < 1:\n",
        "  PREFIX = pdb_file.split('.')[0]\n",
        "else:\n",
        "  PREFIX = os.path.join('/content/', PREFIX)\n",
        "\n",
        "full_fname = PREFIX + '.csv'\n",
        "\n",
        "if VERBOSE:\n",
        "  if Model == 'Single':\n",
        "    df['Wildtype AA'] = df['Mutation'].str[0]\n",
        "    df['Mutant AA'] = df['Mutation'].str[-1]\n",
        "    df['Position'] = df['Mutation'].str[2:-1]\n",
        "    df['Chain'] = df['Mutation'].str[1]\n",
        "\n",
        "  else:\n",
        "    df[['Mutation 1', 'Mutation 2']] = df['Mutation'].str.split(':', n=2, expand=True)\n",
        "    df['Wildtype AA 1'], df['Wildtype AA 2'] = df['Mutation 1'].str[0], df['Mutation 2'].str[0]\n",
        "    df['Mutant AA 1'], df['Mutant AA 2'] = df['Mutation 1'].str[-1], df['Mutation 2'].str[-1]\n",
        "    df['Position 1'], df['Position 2'] = df['Mutation 1'].str[2:-1], df['Mutation 2'].str[2:-1]\n",
        "    df['Chain 1'], df['Chain 2'] = df['Mutation 1'].str[1], df['Mutation 2'].str[1]\n",
        "\n",
        "df.to_csv(full_fname, index=True)\n",
        "files.download(full_fname)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaIXOllfc2Ok"
      },
      "source": [
        "# APPENDIX\n",
        "\n",
        "## License\n",
        "\n",
        "The source code for ThermoMPNN-D, including license information, can be found [here](https://github.com/Kuhlman-Lab/ThermoMPNN-D)\n",
        "\n",
        "---\n",
        "\n",
        "## Citation Information\n",
        "\n",
        "If you use ThermoMPNN or ThermoMPNN-D in your research, please cite the following paper(s):\n",
        "\n",
        "### Epistatic or Additive model:\n",
        "Dieckhaus, H., Kuhlman, B., *Protein stability models fail to capture epistatic interactions of double point mutations*. **2025**, Protein Science, 34(1): e70003, doi: https://doi.org/10.1002/pro.70003.\n",
        "\n",
        "### Single mutant model:\n",
        "Dieckhaus, H., Brocidiacono, M., Randolph, N., Kuhlman, B. *Transfer learning to leverage larger datasets for improved prediction of protein stability changes.* Proc Natl Acad Sci **2024**, 121(6): e2314853121, doi: https://doi.org/10.1073/pnas.2314853121.\n",
        "\n",
        "---\n",
        "\n",
        "## Contact Information\n",
        "\n",
        "Please contact Henry Dieckhaus at dieckhau@unc.edu to report any bugs or issues with this notebook. You may also submit issues on the ThermoMPNN-D GitHub page [here](https://github.com/Kuhlman-Lab/ThermoMPNN-D/issues).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}